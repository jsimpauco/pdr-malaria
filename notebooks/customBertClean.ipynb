{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zsvxdkMmPMsg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 11:45:17.445020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-03 11:45:17.445076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-03 11:45:17.446838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-03 11:45:17.457932: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers, datasets\n",
    "from transformers import Trainer\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from Bio import SeqIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Together, these two functions take a large fasta file and create strings of the nucleotide data contained \n",
    "#within into seperate files for each speciemen of malaria included in the file\n",
    "\n",
    "#RUNNING THESE WILL CREATE FILES, only run if files are missing\n",
    "def _helper(filename = \"../data/Plasmodium_falciparum_3D7_Genome.fasta\"):\n",
    "    #parses file with genome into dictionary format\n",
    "    record_dict = SeqIO.to_dict(SeqIO.parse(filename, \"fasta\"))\n",
    "    for key in record_dict.keys():\n",
    "        yield record_dict[key].seq, key\n",
    "    return \"dictionary ready\"\n",
    "def create_helperdata(CHUNCK_SIZE=512):\n",
    "    for sequence, name in iter(_helper()):\n",
    "        with open(f\"../data/{name}.txt\", \"w\") as f:\n",
    "            chuncks = len(sequence) // CHUNCK_SIZE\n",
    "            for i in range(chuncks):\n",
    "                indx = i*CHUNCK_SIZE\n",
    "                chunck = sequence[indx:indx+CHUNCK_SIZE]\n",
    "                f.write(f\"{chunck}\\n\")\n",
    "    return \"files created\"\n",
    "#_helper(filename = \"../data/Plasmodium_falciparum_3D7_Genome.fasta\")\n",
    "#create_helperdata(CHUNCK_SIZE=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jaQsRw4xC-x"
   },
   "source": [
    "## 1.2 Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1961,
     "status": "ok",
     "timestamp": 1665133698134,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "ER4dwYJDoZFU",
    "outputId": "c0ca8f0c-4610-4dc2-f386-09be4d2f139f"
   },
   "outputs": [],
   "source": [
    "#seed helps us generate the same random shuffle of data\n",
    "seed = 4\n",
    "\n",
    "#from the txt files created from the helper functions above, create pairs of adjacent nucleotides strings for one .txt file\n",
    "def create_onepairs(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "    lines = list(zip(lines[:-1], lines[1:]))\n",
    "    return lines\n",
    "\n",
    "#repeat for each .txt file\n",
    "def create_pairedData(folder_path = \"../data/\"):\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if '.txt' in filename:\n",
    "            f = \"../data/\" + filename\n",
    "            filenames.append(f)\n",
    "    pairs = []\n",
    "    for file in filenames:\n",
    "        pair = create_onepairs(file)\n",
    "        pairs = pairs + pair\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turns input nucleotides into appropriate integer label\n",
    "class tokenizer():\n",
    "    def __init__(self, vocab, special_tokens):\n",
    "        #vocab is given as a list of all words \n",
    "        #but will turn into a dictionary with words as keys and integers as values\n",
    "\n",
    "        #special_tokens are extra symbols that aren't standard words, \n",
    "        #but rather used to delimit or do something within the text\n",
    "\n",
    "        self.vocab = special_tokens + vocab\n",
    "        self.tokens = {}\n",
    "        d = {}\n",
    "        for i in range(len(self.vocab)):\n",
    "            d[self.vocab[i]] = i\n",
    "            self.tokens[i] = self.vocab[i]\n",
    "        self.vocab = d\n",
    "\n",
    "            \n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        #ids are a list of integers that represent the encoded words\n",
    "        #this function will convert each integer into a word\n",
    "        #Ex. ids = [0, 3, 4, 1] dict={0: 'word1', 1: 'word2', 2:'word3', 3:'word4', 4:'word5'}\n",
    "        self.output_ids = []\n",
    "        for oneid in ids:\n",
    "            if oneid not in self.tokens.keys():\n",
    "                self.input_ids.append(self.tokens['[UNK]'])\n",
    "                continue\n",
    "            self.output_ids.append(self.tokens[oneid])\n",
    "        return self.output_ids\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        #sequence is a list of nucleotides\n",
    "        #use tokenizer on a sequence of nucleotides to output the resulting integers that are mapped to each nucleotide\n",
    "        self.input_ids = {'input_ids':[]}\n",
    "        if not isinstance(sequence, list):\n",
    "            sequence = [sequence]\n",
    "        for nucleotide in sequence:\n",
    "            if nucleotide not in self.vocab.keys():\n",
    "                self.input_ids['input_ids'].append(self.vocab['[UNK]'])\n",
    "                continue\n",
    "            self.input_ids['input_ids'].append(self.vocab[nucleotide])\n",
    "        return self.input_ids\n",
    "\n",
    "t = tokenizer(vocab=['A', 'T', 'G', 'C'], \n",
    "          special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]'])     \n",
    "\n",
    "MAX_LEN = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE CODE BELOW WAS TAKEN AND MODIFIED FROM USER CHEEKAN on Medium, \n",
    "#https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbBU9BTdvlQ0"
   },
   "source": [
    "# 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bXCsfR3tmajw"
   },
   "outputs": [],
   "source": [
    "#takes in paired nucleotide data and tokenizes and tensorizes it to prepare for training or prediction. Will mask portions \n",
    "# of the nucleotide sequence when training\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=1024, is_train=False):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: replace random words in sentence with mask / random words\\\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "            \n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "        # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = list(sentence)#sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "\n",
    "            # remove cls and sep token\n",
    "            #token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            # 15% chance of altering token\n",
    "            if prob < 0.15 and self.is_train:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(self.tokenizer(token)[\"input_ids\"])\n",
    "\n",
    "                output_label.append(self.tokenizer(token)[\"input_ids\"])\n",
    "\n",
    "            else:\n",
    "                output.append(self.tokenizer(token)[\"input_ids\"])\n",
    "                output_label.append(self.tokenizer(token)[\"input_ids\"])\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        '''return random sentence pair'''\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        '''return sentence pair'''\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        '''return random single sentence'''\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT-uR5BXChcM"
   },
   "source": [
    "# 3) Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 804,
     "status": "ok",
     "timestamp": 1665133801248,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "S6Nr8gMt49dF",
    "outputId": "e834d002-629f-4d93-a5dd-4e56bf07ec1f"
   },
   "outputs": [],
   "source": [
    "### Creates embeddings/ associates values with the value, position, and segment of each nucleotide in a sequence\n",
    "#Basically gives mathematical values to where and what each nucleotide in a sequence so that they can be mathematically computed.\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        pe = torch.zeros(max_len, d_model).float().to(self.device)\n",
    "        pe.require_grad = False\n",
    "\n",
    "        for pos in range(max_len):\n",
    "            # for each dimension of the each position\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        # include the batch size\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        # self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe\n",
    "\n",
    "class BERTEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, seq_len=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embed_size = embed_size\n",
    "        # (m, seq_len) --> (m, seq_len, embed_size)\n",
    "        # padding_idx is not updated during training, remains as fixed pad (0)\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0, device=self.device)\n",
    "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0, device=self.device)\n",
    "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, sequence, segment_label=None, is_train=False):\n",
    "        x = self.token(sequence) + self.position(sequence)\n",
    "        if segment_label is not None:\n",
    "            x += self.segment(segment_label)\n",
    "\n",
    "        if is_train == False:\n",
    "            return x\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1665133802282,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "g7n6FOkOBWK2",
    "outputId": "61857fc3-c499-4a4f-a97d-2d5af268e2b1"
   },
   "outputs": [],
   "source": [
    "### attention layers\n",
    "#creates a matrix where each row represents the word being queried and the column represents the word whose relationship value\n",
    "#is being calculated so that words which are highly related have a high value, and words that are not related have a low value\n",
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "\n",
    "        assert d_model % heads == 0\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.key = torch.nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.value = torch.nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, device=self.device)\n",
    "\n",
    "    def forward(self, query, key, value, mask, is_train=False):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, d_model)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        mask = mask.float()\n",
    "        mask = (1 - mask) * -1e9 \n",
    "        scores = scores + mask\n",
    "\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        if is_train == True:\n",
    "            weights = self.dropout(weights) \n",
    "\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"Implements FFN equation\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim, device=self.device)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model, device=self.device)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x, is_train=False):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        if is_train == True:\n",
    "            out = self.fc2(self.dropout(out))\n",
    "        else:\n",
    "            out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "#Uses multihead attention to calculate the relationship of each word in the input sequence with each other word\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=768,\n",
    "        heads=12,\n",
    "        feed_forward_hidden=768 * 4,\n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model, device = self.device)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask, is_train=False):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        \n",
    "        if is_train == True:\n",
    "            interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "            interacted = self.layernorm(interacted + embeddings)\n",
    "            feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        else: \n",
    "            interacted = self.self_multihead(embeddings, embeddings, embeddings, mask)\n",
    "            interacted = self.layernorm(interacted + embeddings)\n",
    "            feed_forward_out = self.feed_forward(interacted)\n",
    "    \n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        \n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15539,
     "status": "ok",
     "timestamp": 1665133817819,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "T4am76N6Cimj",
    "outputId": "e0b849c5-50c2-4f15-b1f0-f08fbe95fc90"
   },
   "outputs": [],
   "source": [
    "#Combines each layer above to create predictions based on the values calculated by each layer\n",
    "class BERT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # paper noted they used 4*hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info=None, is_train=False):\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x\n",
    "\n",
    "class NextSentencePrediction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "        self.linear = torch.nn.Linear(hidden, 2, device=self.device)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, is_train=False):\n",
    "        # use only the first token which is the [CLS]\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "class MaskedLanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size, device=self.device)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, is_train=False):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "class BERTLM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label=None, is_train=False):\n",
    "        x = self.bert(x, segment_label)\n",
    "        if segment_label is not None:\n",
    "            return self.next_sentence(x), self.mask_lm(x)\n",
    "        else:\n",
    "            return self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnp98JEZWwgN"
   },
   "source": [
    "# 4) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0PPi4L1sCjBf"
   },
   "outputs": [],
   "source": [
    "### optimizer\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEggGpx2kUXx",
    "outputId": "578f0c3c-930b-41a8-8b3c-592d3888ecac"
   },
   "outputs": [],
   "source": [
    "### trainer\n",
    "#creates models that are around 300 mb in size, so they can't be pushed to github\n",
    "class BERTTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader=None,\n",
    "        lr= 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        warmup_steps=10000,\n",
    "        log_freq=10,\n",
    "        device='cuda'\n",
    "        ):\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n",
    "            )\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        for i, data in data_iter:\n",
    "\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            in_bert = data[\"bert_input\"]\n",
    "            segmentlabel_bert = data[\"segment_label\"]\n",
    "            is_next_bert = data[\"is_next\"]\n",
    "            label_bert = data[\"bert_label\"]\n",
    "            \n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(in_bert, segmentlabel_bert, is_train=True)\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, is_next_bert)\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
    "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), label_bert)\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = mask_loss#next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(is_next_bert).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += is_next_bert.nelement()\n",
    "\n",
    "            # step 4: validation step\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "        print(\n",
    "            f\"EP{epoch}, {mode}: \\\n",
    "            avg_loss={avg_loss / len(data_iter)}, \\\n",
    "            total_acc={total_correct * 100.0 / total_element}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data from FASTA file into lists of nucleotides\n",
    "pairs = create_pairedData()\n",
    "random.Random(seed).shuffle(pairs)\n",
    "\n",
    "training_size = int(len(pairs)*0.8)\n",
    "\n",
    "#Prepare data from lists of nucleotides into tensored and tokenized data ready to be batched and trained\n",
    "train_data = BERTDataset(pairs[:training_size], seq_len=MAX_LEN, tokenizer=t)\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, pin_memory=True)\n",
    "bert_model = BERT(len(t.vocab))\n",
    "bert_lm = BERTLM(bert_model, len(t.vocab))\n",
    "bert_trainer = BERTTrainer(bert_lm, train_loader, device='cuda')\n",
    "epochs = 20\n",
    "\n",
    "#For every specified number of epochs, save the current version of the model as a file to the filepath\n",
    "for epoch in range(epochs):\n",
    "    bert_trainer.train(epoch)\n",
    "    if epoch % 1 == 0 and epoch != 0:\n",
    "        filename = \"../model/largeMalariaModelEpoch\" + str(epoch)\n",
    "        torch.save(bert_trainer.model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't script anything below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(bert_trainer.model.state_dict(), \"../model/small_test_model\")\n",
    "#model = BERTLM(bert_model, len(t.vocab))\n",
    "#model.load_state_dict(torch.load(\"../model/malariaModelEpoch9\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_data = BERTDataset(pairs[5:10], seq_len=MAX_LEN, tokenizer=t, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BERT(len(t.vocab))\n",
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/largeMalariaModelEpoch1\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = create_pairedData()\n",
    "random.Random(seed).shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(pairs)*0.8)\n",
    "val_data = BERTDataset(pairs[training_size:], seq_len=MAX_LEN, tokenizer=t, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(val_data, model_name, tokenizer):\n",
    "    #bert_model = BERT(len(t.vocab))\n",
    "    #model = BERTLM(bert_model, len(t.vocab))\n",
    "    #model.load_state_dict(torch.load(\"../model/\"+model_name, map_location=torch.device('cpu')))\n",
    "    sequences = []\n",
    "    predictions = []\n",
    "    accuracies = []\n",
    "    print('loading in data')\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ind = 0\n",
    "    for data in val_data:\n",
    "        if ind % 1000 == 0:\n",
    "            print(ind)\n",
    "        in_bert = data[\"bert_input\"].to(device)\n",
    "        #segmentlabel_bert = data[\"segment_label\"].to(device)\n",
    "        #is_next_bert = data[\"is_next\"].to(device)\n",
    "        #label_bert = data[\"bert_label\"].to(device)\n",
    "        \n",
    "        #turns tensorized and tokenized input back into readable nucleotides\n",
    "        seq = t.convert_ids_to_tokens(in_bert.tolist())\n",
    "        sequences.append(seq)\n",
    "\n",
    "        #turns tensorized and tokenized output back into readable nucleotides\n",
    "        output = model(in_bert.reshape(1,-1))\n",
    "        #return output\n",
    "        #.max grabs the option with the largest weight\n",
    "        tensor_pred = torch.max(output[:512], axis=-1)[1]\n",
    "        pred = tensor_pred.tolist()\n",
    "        pred = sum(pred, [])\n",
    "        pred = t.convert_ids_to_tokens(pred)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        num_correct = 0\n",
    "        for i in range(len(seq)):\n",
    "            if seq[i] == pred[i]:\n",
    "                num_correct = num_correct + 1\n",
    "        acc = num_correct / len(seq)\n",
    "        accuracies.append(acc)\n",
    "        ind = ind + 1\n",
    "    acc_df = {'sequence': sequences, 'prediction': predictions, 'accuracy': accuracies}\n",
    "    acc_df = pd.DataFrame.from_dict(acc_df)\n",
    "    return acc_df#len(sequences), len(predictions), len(accuracies)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "df1 = calc_acc(val_data, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.539062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.420898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.384766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.389648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.303711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.358398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.399414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.410156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.458008  \n",
       "1     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.539062  \n",
       "2     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.420898  \n",
       "3     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.384766  \n",
       "4     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.389648  \n",
       "...                                                 ...       ...  \n",
       "9105  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.303711  \n",
       "9106  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.358398  \n",
       "9107  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.453125  \n",
       "9108  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.399414  \n",
       "9109  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.410156  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/largeMalariaModelEpoch2\", map_location=torch.device('cpu')))\n",
    "df2 = calc_acc(val_data, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/smallMalariaModelEpoch10\", map_location=torch.device('cpu')))\n",
    "df3 = calc_acc(val_data, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/smallMalariaModelEpoch20\", map_location=torch.device('cpu')))\n",
    "df4 = calc_acc(val_data, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/smallMalariaModelEpoch30\", map_location=torch.device('cpu')))\n",
    "df5 = calc_acc(val_data, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.405273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.294922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.428711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.434570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.427734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.436523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.457031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.404297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.351562  \n",
       "1     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.405273  \n",
       "2     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.294922  \n",
       "3     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.428711  \n",
       "4     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.434570  \n",
       "...                                                 ...       ...  \n",
       "9105  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.458008  \n",
       "9106  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.427734  \n",
       "9107  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.436523  \n",
       "9108  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.457031  \n",
       "9109  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.404297  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[G, A, C, A, T, A, A, T, A, T, A, T, A, T, A, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[G, A, T, A, A, A, A, T, G, T, T, A, T, T, T, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[G, T, G, A, T, T, G, T, T, T, A, A, T, T, T, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[G, A, A, A, A, G, T, A, T, A, T, A, A, A, G, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[G, T, T, G, G, A, G, T, A, A, T, G, T, A, T, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[G, T, T, T, T, G, A, T, G, G, A, G, T, T, T, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[G, G, A, G, T, G, T, C, T, C, C, A, T, A, T, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[G, A, C, A, T, A, T, T, T, C, A, A, C, A, C, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[G, T, T, T, A, T, T, T, C, T, T, C, T, A, A, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[G, T, T, A, T, G, A, A, G, G, G, A, A, A, T, ...</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [G, A, C, A, T, A, A, T, A, T, A, T, A, T, A, ...  0.998047  \n",
       "1     [G, A, T, A, A, A, A, T, G, T, T, A, T, T, T, ...  0.998047  \n",
       "2     [G, T, G, A, T, T, G, T, T, T, A, A, T, T, T, ...  0.998047  \n",
       "3     [G, A, A, A, A, G, T, A, T, A, T, A, A, A, G, ...  0.998047  \n",
       "4     [G, T, T, G, G, A, G, T, A, A, T, G, T, A, T, ...  0.998047  \n",
       "...                                                 ...       ...  \n",
       "9105  [G, T, T, T, T, G, A, T, G, G, A, G, T, T, T, ...  0.998047  \n",
       "9106  [G, G, A, G, T, G, T, C, T, C, C, A, T, A, T, ...  0.998047  \n",
       "9107  [G, A, C, A, T, A, T, T, T, C, A, A, C, A, C, ...  0.998047  \n",
       "9108  [G, T, T, T, A, T, T, T, C, T, T, C, T, A, A, ...  0.998047  \n",
       "9109  [G, T, T, A, T, G, A, A, G, G, G, A, A, A, T, ...  0.998047  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.349609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.450195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.294922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.464844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.434570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.427734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.346680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.457031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "      <td>0.369141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.349609  \n",
       "1     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.450195  \n",
       "2     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.294922  \n",
       "3     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.464844  \n",
       "4     [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.434570  \n",
       "...                                                 ...       ...  \n",
       "9105  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.458008  \n",
       "9106  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.427734  \n",
       "9107  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.346680  \n",
       "9108  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.457031  \n",
       "9109  [T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, ...  0.369141  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.525391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.384766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.365234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.303711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.391602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.376953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.397461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.483398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.458008  \n",
       "1     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.550781  \n",
       "2     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.525391  \n",
       "3     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.384766  \n",
       "4     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.365234  \n",
       "...                                                 ...       ...  \n",
       "9105  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.303711  \n",
       "9106  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.391602  \n",
       "9107  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.376953  \n",
       "9108  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.397461  \n",
       "9109  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.483398  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.567383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.513672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.362305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.405273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.303711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.373047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.357422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.483398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.458008  \n",
       "1     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.567383  \n",
       "2     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.513672  \n",
       "3     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.362305  \n",
       "4     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.405273  \n",
       "...                                                 ...       ...  \n",
       "9105  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.303711  \n",
       "9106  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.373047  \n",
       "9107  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.453125  \n",
       "9108  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.357422  \n",
       "9109  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.483398  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/smallMalariaModelEpoch40\", map_location=torch.device('cpu')))\n",
    "df6 = calc_acc(val_data, model, t)\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.524414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.473633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.370117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.365234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>[[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.270508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>[[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.358398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>[[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.432617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>[[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.355469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>[[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "      <td>0.462891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "0     [[CLS], A, C, A, T, A, A, T, A, T, A, T, A, T,...   \n",
       "1     [[CLS], A, T, A, A, A, A, T, G, T, T, A, T, T,...   \n",
       "2     [[CLS], T, G, A, T, T, G, T, T, T, A, A, T, T,...   \n",
       "3     [[CLS], A, A, A, A, G, T, A, T, A, T, A, A, A,...   \n",
       "4     [[CLS], T, T, G, G, A, G, T, A, A, T, G, T, A,...   \n",
       "...                                                 ...   \n",
       "9105  [[CLS], T, T, T, T, G, A, T, G, G, A, G, T, T,...   \n",
       "9106  [[CLS], G, A, G, T, G, T, C, T, C, C, A, T, A,...   \n",
       "9107  [[CLS], A, C, A, T, A, T, T, T, C, A, A, C, A,...   \n",
       "9108  [[CLS], T, T, T, A, T, T, T, C, T, T, C, T, A,...   \n",
       "9109  [[CLS], T, T, A, T, G, A, A, G, G, G, A, A, A,...   \n",
       "\n",
       "                                             prediction  accuracy  \n",
       "0     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.458008  \n",
       "1     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.524414  \n",
       "2     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.473633  \n",
       "3     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.370117  \n",
       "4     [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.365234  \n",
       "...                                                 ...       ...  \n",
       "9105  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.270508  \n",
       "9106  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.358398  \n",
       "9107  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.432617  \n",
       "9108  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.355469  \n",
       "9109  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  0.462891  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/smallMalariaModelEpoch50\", map_location=torch.device('cpu')))\n",
    "df7 = calc_acc(val_data, model, t)\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40219871278128433"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>avg_acc</th>\n",
       "      <th>min_acc</th>\n",
       "      <th>max_acc</th>\n",
       "      <th>min_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>largeMalariaModelEpoch1</td>\n",
       "      <td>0.402199</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>largeMalariaModelEpoch2</td>\n",
       "      <td>0.403329</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.638672</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smallMalariaModelEpoch10</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model_name   avg_acc   min_acc   max_acc  min_per_epoch\n",
       "0   largeMalariaModelEpoch1  0.402199  0.117188  0.578125            160\n",
       "1   largeMalariaModelEpoch2  0.403329  0.109375  0.638672            160\n",
       "2  smallMalariaModelEpoch10  0.998047  0.998047  0.998047              6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = [\"largeMalariaModelEpoch1\", \"largeMalariaModelEpoch2\", \"smallMalariaModelEpoch10\"]\n",
    "avg_acc = [df1['accuracy'].mean(), df2['accuracy'].mean(), df3['accuracy'].mean()]\n",
    "min_acc = [df1['accuracy'].min(), df2['accuracy'].min(), df3['accuracy'].min()]\n",
    "max_acc = [df1['accuracy'].max(), df2['accuracy'].max(), df3['accuracy'].max()]\n",
    "min_per_epoch = [160, 160, 6]\n",
    "summ_d = {'model_name': model_name, 'avg_acc': avg_acc, 'min_acc': min_acc, 'max_acc': max_acc, 'min_per_epoch': min_per_epoch}\n",
    "summ_df = pd.DataFrame.from_dict(summ_d)\n",
    "summ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99804688])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['accuracy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTLM(bert_model, len(t.vocab))\n",
    "model.load_state_dict(torch.load(\"../model/smallMalariaModelEpoch10\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "inp_ds = BERTDataset(pairs[-1], seq_len=MAX_LEN, tokenizer=t, is_train=False)\n",
    "inp = inp_ds[0][\"bert_input\"].to(device)\n",
    "output = model(inp.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3943e+00, -2.4529e+00, -2.5351e+00,  ..., -2.1107e+00,\n",
       "          -1.7129e+00, -1.8571e+00],\n",
       "         [-8.7544e+00, -8.7423e+00, -8.7755e+00,  ..., -1.0087e-03,\n",
       "          -9.8180e+00, -9.4055e+00],\n",
       "         [-2.2688e+00, -2.2952e+00, -2.5661e+00,  ..., -2.5384e+00,\n",
       "          -1.9775e+00, -2.1003e+00],\n",
       "         ...,\n",
       "         [-3.0607e+00, -3.0230e+00, -3.1944e+00,  ..., -1.8140e+00,\n",
       "          -1.6713e+00, -1.3568e+00],\n",
       "         [-3.0451e+00, -3.0082e+00, -3.1700e+00,  ..., -1.6946e+00,\n",
       "          -1.8494e+00, -1.3074e+00],\n",
       "         [-3.0222e+00, -2.9848e+00, -3.1288e+00,  ..., -1.5711e+00,\n",
       "          -2.0388e+00, -1.3199e+00]]], device='cuda:0',\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(output, dim=-1)\n",
    "probs = probs[0][20:30]\n",
    "probs = probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_probs = [prob[-4:] for prob in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list, flattening each sublist into rows\n",
    "prob_df = pd.DataFrame(new_probs,columns=['A', 'T', 'G', 'C'])\n",
    "prob_df['total'] = prob_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df['A'] = prob_df['A'] / prob_df['total']\n",
    "prob_df['T'] = prob_df['T'] / prob_df['total']\n",
    "prob_df['G'] = prob_df['G'] / prob_df['total']\n",
    "prob_df['C'] = prob_df['C'] / prob_df['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = prob_df.drop(columns=['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df['sequence_position'] = list(range(20, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob_df.plot(x='sequence_position', kind='bar', stacked=True,\n",
    "        title='Nucleotide Probabilities by Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>T</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>sequence_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333806</td>\n",
       "      <td>0.179797</td>\n",
       "      <td>0.125816</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.324118</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>0.131340</td>\n",
       "      <td>0.362059</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.301417</td>\n",
       "      <td>0.199514</td>\n",
       "      <td>0.138699</td>\n",
       "      <td>0.360370</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.284580</td>\n",
       "      <td>0.219090</td>\n",
       "      <td>0.144285</td>\n",
       "      <td>0.352044</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.285880</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.147112</td>\n",
       "      <td>0.337737</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.304728</td>\n",
       "      <td>0.225531</td>\n",
       "      <td>0.148845</td>\n",
       "      <td>0.320897</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.328454</td>\n",
       "      <td>0.212723</td>\n",
       "      <td>0.151502</td>\n",
       "      <td>0.307321</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.340441</td>\n",
       "      <td>0.199123</td>\n",
       "      <td>0.157158</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.332209</td>\n",
       "      <td>0.189469</td>\n",
       "      <td>0.167299</td>\n",
       "      <td>0.311022</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.310597</td>\n",
       "      <td>0.183221</td>\n",
       "      <td>0.180107</td>\n",
       "      <td>0.326074</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         T         G         C  sequence_position\n",
       "0  0.333806  0.179797  0.125816  0.360581                 20\n",
       "1  0.324118  0.182482  0.131340  0.362059                 21\n",
       "2  0.301417  0.199514  0.138699  0.360370                 22\n",
       "3  0.284580  0.219090  0.144285  0.352044                 23\n",
       "4  0.285880  0.229270  0.147112  0.337737                 24\n",
       "5  0.304728  0.225531  0.148845  0.320897                 25\n",
       "6  0.328454  0.212723  0.151502  0.307321                 26\n",
       "7  0.340441  0.199123  0.157158  0.303278                 27\n",
       "8  0.332209  0.189469  0.167299  0.311022                 28\n",
       "9  0.310597  0.183221  0.180107  0.326074                 29"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
