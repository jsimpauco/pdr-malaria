{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1100e809-58a4-44db-b266-7a17d653ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "91f27bd9-965d-4f72-9947-73b50d75a2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b1a9e5c-710d-4f96-8554-867cc4cf4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "    return lines\n",
    "\n",
    "def create_sequences(folder_path):\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if '.txt' in filename:\n",
    "            f = \"../../data/\" + filename\n",
    "            filenames.append(f)\n",
    "    lines = []\n",
    "    for file in filenames:\n",
    "        line = get_sequences(file)\n",
    "        lines = lines + line\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18926632-ca74-45d7-8fdd-494a64fcf678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(folder_path, data):\n",
    "    l = len(data)\n",
    "    labels = [random.randint(0, 1) for _ in range(l)]\n",
    "    num_train = int(l * 0.8)\n",
    "    num_test = ((l - num_train) // 2) + num_train\n",
    "    \n",
    "    train_dict = {'sequence': data[:num_train], 'label': labels[:num_train]}\n",
    "    test_dict ={'sequence': data[num_train:num_test], 'label': labels[num_train:num_test]}\n",
    "    dev_dict = {'sequence': data[num_test:], 'label': labels[num_test:]}\n",
    "    \n",
    "    train_df = pd.DataFrame(train_dict)\n",
    "    test_df = pd.DataFrame(test_dict)\n",
    "    dev_df = pd.DataFrame(dev_dict)\n",
    "\n",
    "    train_path = folder_path + '/train.csv'\n",
    "    test_path = folder_path + '/test.csv'\n",
    "    dev_path = folder_path + '/dev.csv'\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    dev_df.to_csv(dev_path, index=False)\n",
    "    return 'csv files created'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "edd86a76-8ed2-47b6-97be-d6124209b7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'csv files created'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this creates csv files, only run once if you don't have files\n",
    "#k = create_sequences(folder_path = \"../../data\")\n",
    "#create_csv(\"../../data/finetuning_data\", k[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "17d80818-51c7-43a3-8589-29fc788fe288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6970db4c85d4aa997d7fa6b05120361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flash_attn_triton.py:   0%|          | 0.00/42.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/zhihan1996/DNABERT-2-117M:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa89d43bcb4428c933f97d134b0db85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bert_padding.py:   0%|          | 0.00/6.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/zhihan1996/DNABERT-2-117M:\n",
      "- bert_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/zhihan1996/DNABERT-2-117M:\n",
      "- flash_attn_triton.py\n",
      "- bert_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "2025-02-12 12:41:22.294795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model class you are passing has a `config_class` attribute that is not consistent with the config class you passed (model has <class 'transformers.models.bert.configuration_bert.BertConfig'> and you passed <class 'transformers_modules.zhihan1996.DNABERT-2-117M.d064dece8a8b41d9fb8729fbe3435278786931f1.configuration_bert.BertConfig'>. Fix one of those so they match!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzhihan1996/DNABERT-2-117M\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzhihan1996/DNABERT-2-117M\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:557\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    554\u001b[0m     class_ref, pretrained_model_name_or_path, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    556\u001b[0m _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    560\u001b[0m     pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    561\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:584\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.register\u001b[0;34m(cls, config_class, model_class, exist_ok)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mRegister a new model for this class.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m        The model to register.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_class, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_class\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(model_class\u001b[38;5;241m.\u001b[39mconfig_class) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mstr\u001b[39m(config_class):\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model class you are passing has a `config_class` attribute that is not consistent with the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig class you passed (model has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39mconfig_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and you passed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Fix \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone of those so they match!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m     )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mregister(config_class, model_class, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n",
      "\u001b[0;31mValueError\u001b[0m: The model class you are passing has a `config_class` attribute that is not consistent with the config class you passed (model has <class 'transformers.models.bert.configuration_bert.BertConfig'> and you passed <class 'transformers_modules.zhihan1996.DNABERT-2-117M.d064dece8a8b41d9fb8729fbe3435278786931f1.configuration_bert.BertConfig'>. Fix one of those so they match!"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d1afa-530c-40ba-8932-1e9d5eddd825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
